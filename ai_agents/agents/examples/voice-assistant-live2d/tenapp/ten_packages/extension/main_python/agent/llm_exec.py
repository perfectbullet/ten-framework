#
# This file is part of TEN Framework, an open source project.
# Licensed under the Apache License, Version 2.0.
# See the LICENSE file for more information.
#
import asyncio
import json
import traceback
from typing import Awaitable, Callable, Literal, Optional
from ten_ai_base.const import CMD_PROPERTY_RESULT
from ten_ai_base.helper import AsyncQueue
from ten_ai_base.struct import (
    LLMMessage,
    LLMMessageContent,
    LLMMessageFunctionCall,
    LLMMessageFunctionCallOutput,
    LLMRequest,
    LLMResponse,
    LLMResponseMessageDelta,
    LLMResponseMessageDone,
    LLMResponseReasoningDelta,
    LLMResponseReasoningDone,
    LLMResponseToolCall,
    parse_llm_response,
)
from ten_ai_base.types import LLMToolMetadata, LLMToolResult
from ..helper import _send_cmd, _send_cmd_ex
from ten_runtime import AsyncTenEnv, Loc, StatusCode
import uuid

from .Retrieval_RAGFlow_Client import RAGFlowRetrievalClient


class LLMExec:
    """
    Context for LLM operations, including ASR and TTS.
    This class handles the interaction with the LLM, including processing commands and data.
    """

    def __init__(self, ten_env: AsyncTenEnv):
        self.ten_env = ten_env
        self.input_queue = AsyncQueue()# ç”¨æˆ·è¾“å…¥é˜Ÿåˆ—
        self.stopped = False
        # ========== å›è°ƒå‡½æ•° ==========
        self.on_response: Optional[
            Callable[[AsyncTenEnv, str, str, bool], Awaitable[None]]
        ] = None
        # æ¨ç†å›è°ƒ
        self.on_reasoning_response: Optional[
            Callable[[AsyncTenEnv, str, str, bool], Awaitable[None]]
        ] = None
        # æ­£å¸¸å“åº”å›è°ƒ
        self.on_tool_call: Optional[
            Callable[[AsyncTenEnv, LLMToolMetadata], Awaitable[None]]
        ] = None
        self.current_task: Optional[asyncio.Task] = None
        self.loop = asyncio.get_event_loop()
        # å¯åŠ¨è¾“å…¥é˜Ÿåˆ—å¤„ç†
        self.loop.create_task(self._process_input_queue())
        # ========== å·¥å…·æ³¨å†Œè¡¨ ==========
        self.available_tools: list[LLMToolMetadata] = []
        self.tool_registry: dict[str, str] = {}
        self.available_tools_lock = (
            asyncio.Lock()
        )  # Lock to ensure thread-safe access
        # ========== å¯¹è¯ä¸Šä¸‹æ–‡ ==========
        self.contexts: list[LLMMessage] = []
        # ç¤ºä¾‹ç»“æ„:
        # [
        #   {"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·?"},
        #   {"role": "assistant", "content": "ä»Šå¤©åŒ—äº¬æ™´å¤©"},
        #   {"role": "user", "content": "é‚£æ˜å¤©å‘¢?"}
        # ]
        self.current_request_id: Optional[str] = None
        self.current_text = None

    async def queue_input(self, item: str) -> None:
        await self.input_queue.put(item)

    async def flush(self) -> None:
        """
        Flush the input queue to ensure all items are processed.
        This is useful for ensuring that all pending inputs are handled before stopping.
        """
        await self.input_queue.flush()
        if self.current_request_id:
            request_id = self.current_request_id
            self.current_request_id = None
            await _send_cmd(
                self.ten_env, "abort", "llm", {"request_id": request_id}
            )
        if self.current_task:
            self.current_task.cancel()

    async def stop(self) -> None:
        """
        Stop the LLMExec processing.
        This will stop the input queue processing and any ongoing tasks.
        """
        self.stopped = True
        await self.flush()
        if self.current_task:
            self.current_task.cancel()

    async def register_tool(self, tool: LLMToolMetadata, source: str) -> None:
        """
        Register tools with the LLM.
        This method sends a command to register the provided tools.
        """
        async with self.available_tools_lock:
            self.available_tools.append(tool)
            self.tool_registry[tool.name] = source

    async def _process_input_queue(self):
        """
        Process the input queue for commands and data.
        This method runs in a loop, processing items from the queue.
        å¤„ç†ç”¨æˆ·è¾“å…¥é˜Ÿåˆ—

        å·¥ä½œæµç¨‹:
        1. ä»é˜Ÿåˆ—å–å‡ºç”¨æˆ·æ–‡æœ¬
        2. åŒ…è£…æˆ LLMMessageContent
        3. å‘é€åˆ° LLM
        4. ç­‰å¾…æµå¼å“åº”
        """
        while not self.stopped:
            try:
                text = await self.input_queue.get()     # é˜»å¡ç­‰å¾…
                new_message = LLMMessageContent(role="user", content=text)

                # åˆ›å»ºä»»åŠ¡å¹¶ç­‰å¾…
                self.current_task = self.loop.create_task(
                    self._send_to_llm(self.ten_env, new_message)
                )
                await self.current_task

            except asyncio.CancelledError:
                self.ten_env.log_info("LLMExec processing cancelled.")
                text = self.current_text
                self.current_text = None
                if self.on_response and text:
                    await self.on_response(self.ten_env, "", text, True)
            except Exception as e:
                self.ten_env.log_error(
                    f"Error processing input queue: {traceback.format_exc()}"
                )
            finally:
                self.current_task = None

    async def _queue_context(
        self, ten_env: AsyncTenEnv, new_message: LLMMessage
    ) -> None:
        """
        Queue a new message to the LLM context.
        This method appends the new message to the existing context and sends it to the LLM.
        """
        ten_env.log_info(f"_queue_context: {new_message}")
        self.contexts.append(new_message)

    async def _write_context(
        self,
        ten_env: AsyncTenEnv,
        role: Literal["user", "assistant"],
        content: str,
    ) -> None:
        last_context = self.contexts[-1] if self.contexts else None
        if last_context and last_context.role == role:
            # If the last context has the same role, append to its content
            last_context.content = content
        else:
            # Otherwise, create a new context message
            new_message = LLMMessageContent(role=role, content=content)
            await self._queue_context(ten_env, new_message)

    async def _send_to_llm(
        self, ten_env: AsyncTenEnv, new_message: LLMMessage
    ) -> None:
        """
        å‘é€æ¶ˆæ¯åˆ° LLM å¹¶å¤„ç†æµå¼å“åº”
        æ­¥éª¤:
        1. åˆå¹¶ä¸Šä¸‹æ–‡ + æ–°æ¶ˆæ¯
        2. æ„é€  LLMRequest (åŒ…å«å·¥å…·åˆ—è¡¨)
        3. è°ƒç”¨ LLM Extension
        4. æµå¼å¤„ç†å“åº”
        """

        # ===== æ–°å¢:RAG æ£€ç´¢ =====
        ten_env.log_info(
            f"_send_to_llm: new_message {new_message}"
        )
        if new_message.role == "user":
            retrieved_docs = await self._retrieve_relevant_docs(new_message.content)
            if retrieved_docs:
                # å°†æ£€ç´¢ç»“æœæ³¨å…¥æ¶ˆæ¯
                enriched_content = self._enrich_with_context(
                    new_message.content,
                    retrieved_docs
                )
                new_message.content = enriched_content
        # ===== RAG æ£€ç´¢ç»“æŸ =====

        # Step 1: åˆå¹¶ä¸Šä¸‹æ–‡
        messages = self.contexts.copy()
        messages.append(new_message)
        # Step 2: æ„é€ è¯·æ±‚
        request_id = str(uuid.uuid4())
        self.current_request_id = request_id
        llm_input = LLMRequest(
            request_id=request_id,
            messages=messages,
            streaming=True,  # å…³é”®: å¯ç”¨æµå¼è¾“å‡º
            parameters={"temperature": 0.7},
            tools=self.available_tools, # ä¼ é€’å·¥å…·åˆ—è¡¨
        )
        input_json = llm_input.model_dump()
        # Step 3: å‘é€å‘½ä»¤
        ten_env.log_info(
            f"_send_to_llm: input_json {input_json}"
        )
        response = _send_cmd_ex(ten_env, "chat_completion", "llm", input_json)
        ten_env.log_info(
            f"_send_to_llm: response {response}"
        )

        # Step 4: å¤„ç†æµå¼å“åº”
        # Queue the new message to the context
        await self._queue_context(ten_env, new_message)     # ä¿å­˜åˆ°ä¸Šä¸‹æ–‡

        async for cmd_result, _ in response:
            if cmd_result and cmd_result.is_final() is False:
                if cmd_result.get_status_code() == StatusCode.OK:
                    response_json, _ = cmd_result.get_property_to_json(None)
                    ten_env.log_info(
                        f"_send_to_llm: response_json {response_json}"
                    )
                    completion = parse_llm_response(response_json)
                    await self._handle_llm_response(completion)

    async def _handle_llm_response(self, llm_output: LLMResponse | None):
        """
        å¤„ç† LLM å“åº” - ä½¿ç”¨ Python 3.10+ çš„æ¨¡å¼åŒ¹é…
        æ”¯æŒçš„å“åº”ç±»å‹:
        1. MessageDelta: æµå¼æ–‡æœ¬å¢é‡
        2. MessageDone: æ–‡æœ¬å®Œæˆ
        3. ReasoningDelta/Done: æ¨ç†è¿‡ç¨‹ (å¦‚ o1 æ¨¡å‹)
        4. ToolCall: å·¥å…·è°ƒç”¨è¯·æ±‚
        """
        self.ten_env.log_info(f"_handle_llm_response: {llm_output}")

        match llm_output:
            # ========== æµå¼æ–‡æœ¬ ==========
            case LLMResponseMessageDelta():
                delta = llm_output.delta    # å¢é‡æ–‡æœ¬
                text = llm_output.content   # ç´¯ç§¯æ–‡æœ¬
                self.current_text = text
                if delta and self.on_response:
                    # è§¦å‘å›è°ƒ â†’ è½¬æ¢ä¸º LLMResponseEvent
                    await self.on_response(self.ten_env, delta, text, False)
                # æ›´æ–°ä¸Šä¸‹æ–‡
                if text:
                    await self._write_context(self.ten_env, "assistant", text)
            # ========== æ–‡æœ¬å®Œæˆ ==========
            case LLMResponseMessageDone():
                text = llm_output.content
                self.current_text = None
                if self.on_response and text:
                    await self.on_response(self.ten_env, "", text, True)
            case LLMResponseReasoningDelta():
                delta = llm_output.delta
                text = llm_output.content
                if delta and self.on_reasoning_response:
                    await self.on_reasoning_response(
                        self.ten_env, delta, text, False
                    )
            # ========== æ¨ç†è¿‡ç¨‹ ==========
            case LLMResponseReasoningDone():
                text = llm_output.content
                if self.on_reasoning_response and text:
                    await self.on_reasoning_response(
                        self.ten_env, "", text, True
                    )
            # ========== å·¥å…·è°ƒç”¨ ==========
            case LLMResponseToolCall():
                self.ten_env.log_info(
                    f"_handle_llm_response: invoking tool call {llm_output.name}"
                )
                src_extension_name = self.tool_registry.get(llm_output.name)
                result, _ = await _send_cmd(
                    self.ten_env,
                    "tool_call",
                    src_extension_name,
                    {
                        "name": llm_output.name,
                        "arguments": llm_output.arguments,
                    },
                )

                if result.get_status_code() == StatusCode.OK:
                    r, _ = result.get_property_to_json(CMD_PROPERTY_RESULT)
                    tool_result: LLMToolResult = json.loads(r)

                    self.ten_env.log_info(f"tool_result: {tool_result}")

                    context_function_call = LLMMessageFunctionCall(
                        name=llm_output.name,
                        arguments=json.dumps(llm_output.arguments),
                        call_id=llm_output.tool_call_id,
                        id=llm_output.response_id,
                        type="function_call",
                    )
                    if tool_result["type"] == "llmresult":
                        result_content = tool_result["content"]
                        if isinstance(result_content, str):
                            await self._queue_context(
                                self.ten_env, context_function_call
                            )
                            await self._send_to_llm(
                                self.ten_env,
                                LLMMessageFunctionCallOutput(
                                    output=result_content,
                                    call_id=llm_output.tool_call_id,
                                    type="function_call_output",
                                ),
                            )
                        else:
                            self.ten_env.log_error(
                                f"Unknown tool result content: {result_content}"
                            )
                    elif tool_result["type"] == "requery":
                        pass
                        # self.memory_cache = []
                        # self.memory_cache.pop()
                        # result_content = tool_result["content"]
                        # nonlocal message
                        # new_message = {
                        #     "role": "user",
                        #     "content": self._convert_to_content_parts(
                        #         message["content"]
                        #     ),
                        # }
                        # new_message["content"] = new_message[
                        #     "content"
                        # ] + self._convert_to_content_parts(
                        #     result_content
                        # )
                        # await self.queue_input_item(
                        #     True, messages=[new_message], no_tool=True
                        # )
                else:
                    self.ten_env.log_error("Tool call failed")

    async def _retrieve_relevant_docs(self, query: str) -> list[str]:
        """
        ä½¿ç”¨ ChromaDB æ£€ç´¢ç›¸å…³æ–‡æ¡£
        """
        try:
            # é…ç½®å®¢æˆ·ç«¯
            client = RAGFlowRetrievalClient(
                base_url="http://192.168.8.231:9380/v1/api/",  # ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…URL
                api_token="ragflow-ZjN2M5MTY2NWJjMzExZjA5Yjg0OTNlMz"  # ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…Token
            )
            # æ‰§è¡Œæ£€ç´¢
            result = client.retrieval(
                kb_id=["02a723a85bc411f09b8493e33f5c065d"],  # ä¿®æ”¹ä¸ºå®é™…çš„çŸ¥è¯†åº“ID, ç›®å‰è¿™ä¸ªidæ˜¯æµ‹è¯•çš„é»˜è®¤çŸ¥è¯†åº“id
                question=query
            )
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if result.get("code") != 0:
                print(f"âŒ é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                print(f"é”™è¯¯ä»£ç : {result.get('code', 'N/A')}")
                return []
            docs = []
            # è§£ææ•°æ®
            data = result.get("data", {})
            chunks = data.get("chunks", [])
            if chunks:
                print("\n" + "-" * 80)
                print("ğŸ” Top æ£€ç´¢ç‰‡æ®µ:")
                for idx, chunk in enumerate(chunks[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª
                    print(f"\nã€ç‰‡æ®µ {idx}ã€‘")
                    print(f"â”œâ”€ æ–‡æ¡£å: {chunk.get('docnm_kwd', 'N/A')}")
                    print(f"â”œâ”€ ç‰‡æ®µID: {chunk.get('chunk_id', 'N/A')}")
                    print(f"â”œâ”€ ç»¼åˆç›¸ä¼¼åº¦: {chunk.get('similarity', 0):.4f}")
                    print(f"â”œâ”€ å‘é‡ç›¸ä¼¼åº¦: {chunk.get('vector_similarity', 0):.4f}")
                    print(f"â”œâ”€ å…³é”®è¯ç›¸ä¼¼åº¦: {chunk.get('term_similarity', 0):.4f}")
                    # æ˜¾ç¤ºå†…å®¹(ä¼˜å…ˆä½¿ç”¨å¸¦æƒé‡çš„å†…å®¹)
                    content = chunk.get('content_with_weight') or chunk.get('content_ltks', '')
                    if content:
                        docs.append(content)
                        # æˆªå–å‰200å­—ç¬¦å¹¶æ¸…ç†æ ¼å¼
                        display_content = content.replace('\n', ' ').strip()[:200]
                        print(f"â””â”€ å†…å®¹é¢„è§ˆ:")
                        print(f"   {display_content}...")
                    else:
                        print(f"â””â”€ å†…å®¹é¢„è§ˆ: (æ— å†…å®¹)")
            return docs
        except Exception as e:
            self.ten_env.log_error(f"ChromaDB retrieval failed: {e}")
        return []

    def _enrich_with_context(self, query: str, docs: list[str]) -> str:
        """å°†æ£€ç´¢ç»“æœæ ¼å¼åŒ–ä¸ºæç¤ºè¯"""
        context = "\n\n".join([f"[æ–‡æ¡£ {i + 1}]\n{doc}" for i, doc in enumerate(docs)])
        return f"""å‚è€ƒä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:

{context}

ç”¨æˆ·é—®é¢˜: {query}"""

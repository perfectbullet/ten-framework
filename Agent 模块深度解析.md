å¥½çš„,æˆ‘æ¥è¯¦ç»†è§£é‡Š `agent` æ¨¡å—çš„ä»£ç æ¶æ„å’Œä½œç”¨,ä»¥åŠå¦‚ä½•ä¸ºé›†æˆ RAG æˆ– MCP åšå‡†å¤‡ã€‚

## Agent æ¨¡å—æ·±åº¦è§£æ

### æ•´ä½“æ¶æ„æ¦‚è§ˆ

Agent æ¨¡å—é‡‡ç”¨ **äº‹ä»¶é©±åŠ¨ + é˜Ÿåˆ—è§£è€¦** çš„è®¾è®¡æ¨¡å¼,æ ¸å¿ƒç›®æ ‡æ˜¯:
1. **è§£è€¦ä¸šåŠ¡é€»è¾‘**: å°† ASRã€LLMã€TTS çš„å¤„ç†é€»è¾‘ä» Extension ä¸­åˆ†ç¦»
2. **å¯ä¸­æ–­æ€§**: æ”¯æŒç”¨æˆ·éšæ—¶æ‰“æ–­ AI å›å¤
3. **æ‰©å±•æ€§**: ä¾¿äºæ·»åŠ æ–°çš„äº‹ä»¶ç±»å‹å’Œå¤„ç†å™¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MainControlExtension                       â”‚
â”‚  (åè°ƒè€… - æ¥æ”¶å¤–éƒ¨äº‹ä»¶,å§”æ‰˜ç»™ Agent å¤„ç†)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ å§”æ‰˜å¤„ç†
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Agent                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  ASR é˜Ÿåˆ—    â”‚  â”‚  LLM é˜Ÿåˆ—    â”‚  â”‚ äº‹ä»¶åˆ†å‘å™¨   â”‚       â”‚
â”‚  â”‚  (é¡ºåºå¤„ç†)  â”‚  â”‚  (å¯ä¸­æ–­)    â”‚  â”‚ (å›è°ƒæ³¨å†Œ)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              LLMExec (LLM æ‰§è¡Œå™¨)                     â”‚   â”‚
â”‚  â”‚  - ä¸Šä¸‹æ–‡ç®¡ç†  - å·¥å…·è°ƒç”¨  - æµå¼å“åº”                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒæ–‡ä»¶è¯¦è§£

### 1. **events.py** - äº‹ä»¶å®šä¹‰å±‚

````python name=agent/events.py
```python
from pydantic import BaseModel
from typing import Literal, Union, Dict, Any

# ========== äº‹ä»¶åŸºç±» ==========
class AgentEventBase(BaseModel):
    """æ‰€æœ‰äº‹ä»¶çš„åŸºç±»,å®šä¹‰äº†äº‹ä»¶çš„ç±»å‹å’Œåç§°"""
    type: Literal["cmd", "data"]  # cmd: å‘½ä»¤äº‹ä»¶, data: æ•°æ®äº‹ä»¶
    name: str  # äº‹ä»¶åç§°

# ========== å‘½ä»¤äº‹ä»¶ (CMD Events) ==========
# è¿™äº›äº‹ä»¶ç”±å¤–éƒ¨ç³»ç»Ÿè§¦å‘,è¡¨ç¤ºç³»ç»ŸçŠ¶æ€å˜åŒ–

class UserJoinedEvent(AgentEventBase):
    """ç”¨æˆ·åŠ å…¥äº‹ä»¶ - å½“ç”¨æˆ·è¿›å…¥æˆ¿é—´æ—¶è§¦å‘"""
    type: Literal["cmd"] = "cmd"
    name: Literal["on_user_joined"] = "on_user_joined"

class UserLeftEvent(AgentEventBase):
    """ç”¨æˆ·ç¦»å¼€äº‹ä»¶ - å½“ç”¨æˆ·ç¦»å¼€æˆ¿é—´æ—¶è§¦å‘"""
    type: Literal["cmd"] = "cmd"
    name: Literal["on_user_left"] = "on_user_left"

class ToolRegisterEvent(AgentEventBase):
    """å·¥å…·æ³¨å†Œäº‹ä»¶ - å½“æ–°å·¥å…·æ³¨å†Œåˆ°ç³»ç»Ÿæ—¶è§¦å‘"""
    type: Literal["cmd"] = "cmd"
    name: Literal["tool_register"] = "tool_register"
    tool: LLMToolMetadata  # å·¥å…·çš„å…ƒæ•°æ®
    source: str  # å·¥å…·æ¥æºçš„ extension åç§°

# ========== æ•°æ®äº‹ä»¶ (DATA Events) ==========
# è¿™äº›äº‹ä»¶æºå¸¦ä¸šåŠ¡æ•°æ®,è¡¨ç¤ºæ•°æ®æµåŠ¨

class ASRResultEvent(AgentEventBase):
    """è¯­éŸ³è¯†åˆ«ç»“æœäº‹ä»¶ - ASR è¾“å‡ºçš„æ–‡æœ¬"""
    type: Literal["data"] = "data"
    name: Literal["asr_result"] = "asr_result"
    text: str  # è¯†åˆ«çš„æ–‡æœ¬
    final: bool  # æ˜¯å¦ä¸ºæœ€ç»ˆç»“æœ (true) è¿˜æ˜¯ä¸­é—´ç»“æœ (false)
    metadata: Dict[str, Any]  # å…ƒæ•°æ® (å¦‚ session_id)

class LLMResponseEvent(AgentEventBase):
    """LLM å“åº”äº‹ä»¶ - LLM è¾“å‡ºçš„æ–‡æœ¬"""
    type: Literal["message", "reasoning"] = "message"  
    # message: æ­£å¸¸å›å¤, reasoning: æ¨ç†è¿‡ç¨‹
    name: Literal["llm_response"] = "llm_response"
    delta: str  # å¢é‡æ–‡æœ¬ (æµå¼è¾“å‡º)
    text: str  # ç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬
    is_final: bool  # æ˜¯å¦ç»“æŸ
```
````

**è®¾è®¡è¦ç‚¹:**
- âœ… **å¼ºç±»å‹å®šä¹‰**: ä½¿ç”¨ Pydantic ç¡®ä¿æ•°æ®ç»“æ„çš„ä¸€è‡´æ€§
- âœ… **åŒºåˆ† CMD å’Œ DATA**: å‘½ä»¤äº‹ä»¶è¡¨ç¤ºçŠ¶æ€å˜åŒ–,æ•°æ®äº‹ä»¶è¡¨ç¤ºæ•°æ®æµ
- ğŸ¯ **æ‰©å±•ç‚¹**: æ·»åŠ  RAG äº‹ä»¶æ—¶,åªéœ€æ–°å¢å¦‚ `RAGQueryEvent`, `RAGResultEvent`

---

### 2. **decorators.py** - è£…é¥°å™¨å·¥å…·

````python name=agent/decorators.py
```python
def agent_event_handler(event_type: AgentEvent):
    """
    è£…é¥°å™¨: æ ‡è®°æ–¹æ³•ä¸ºäº‹ä»¶å¤„ç†å™¨
    
    å·¥ä½œåŸç†:
    1. åœ¨æ–¹æ³•ä¸Šæ·»åŠ  `_agent_event_type` å±æ€§
    2. Agent åˆå§‹åŒ–æ—¶æ‰«ææ‰€æœ‰è£…é¥°çš„æ–¹æ³•
    3. è‡ªåŠ¨æ³¨å†Œåˆ°äº‹ä»¶åˆ†å‘ç³»ç»Ÿ
    
    ç”¨æ³•:
        @agent_event_handler(ASRResultEvent)
        async def on_asr(self, event: ASRResultEvent):
            # å¤„ç† ASR ç»“æœ
    """
    def wrapper(func):
        setattr(func, "_agent_event_type", event_type)
        return func
    return wrapper
```
````

**ä¼˜åŠ¿:**
- ğŸ¨ **å£°æ˜å¼ç¼–ç¨‹**: é€šè¿‡è£…é¥°å™¨æ¸…æ™°è¡¨è¾¾æ„å›¾
- ğŸ”„ **è‡ªåŠ¨æ³¨å†Œ**: æ— éœ€æ‰‹åŠ¨è°ƒç”¨ `agent.on()` æ³¨å†Œ
- ğŸ“ **å¯è¯»æ€§å¼º**: ä¸€çœ¼çœ‹å‡ºå“ªä¸ªæ–¹æ³•å¤„ç†å“ªä¸ªäº‹ä»¶

---

### 3. **agent.py** - æ ¸å¿ƒæ§åˆ¶å™¨

è¿™æ˜¯æœ€æ ¸å¿ƒçš„æ–‡ä»¶,æˆ‘é€æ®µè§£æ:

#### 3.1 åˆå§‹åŒ– - åŒé˜Ÿåˆ—è®¾è®¡

````python name=agent/agent.py (åˆå§‹åŒ–éƒ¨åˆ†)
```python
class Agent:
    def __init__(self, ten_env: AsyncTenEnv):
        self.ten_env = ten_env
        self.stopped = False
        
        # ========== å›è°ƒæ³¨å†Œè¡¨ ==========
        # key: äº‹ä»¶ç±»å‹, value: å¤„ç†å™¨åˆ—è¡¨
        self._callbacks: dict[AgentEvent, list[Callable]] = {}
        
        # ========== åŒé˜Ÿåˆ—è®¾è®¡ ==========
        # ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªé˜Ÿåˆ—?
        # 1. ASR é˜Ÿåˆ—: é¡ºåºå¤„ç†,ä¸å¯ä¸­æ–­ (ä¿è¯ç”¨æˆ·è¾“å…¥çš„å®Œæ•´æ€§)
        # 2. LLM é˜Ÿåˆ—: å¯ä¸­æ–­å¤„ç† (å…è®¸ç”¨æˆ·æ‰“æ–­ AI å›å¤)
        self._asr_queue: asyncio.Queue[ASRResultEvent] = asyncio.Queue()
        self._llm_queue: asyncio.Queue[LLMResponseEvent] = asyncio.Queue()
        
        # æ¶ˆè´¹è€…ä»»åŠ¡
        self._asr_consumer: Optional[asyncio.Task] = None
        self._llm_consumer: Optional[asyncio.Task] = None
        self._llm_active_task: Optional[asyncio.Task] = None  # å½“å‰æ­£åœ¨å¤„ç†çš„ LLM ä»»åŠ¡
        
        # ========== LLM æ‰§è¡Œå™¨ ==========
        self.llm_exec = LLMExec(ten_env)
        # æ³¨å†Œå›è°ƒ: å½“ LLM è¾“å‡ºæ—¶,è½¬æ¢ä¸º LLMResponseEvent
        self.llm_exec.on_response = self._on_llm_response
        self.llm_exec.on_reasoning_response = self._on_llm_reasoning_response
        
        # å¯åŠ¨æ¶ˆè´¹è€…
        self._asr_consumer = asyncio.create_task(self._consume_asr())
        self._llm_consumer = asyncio.create_task(self._consume_llm())
```
````

**æ¶æ„äº®ç‚¹:**
- ğŸ”€ **é˜Ÿåˆ—éš”ç¦»**: ASR å’Œ LLM å¤„ç†äº’ä¸é˜»å¡
- â¸ï¸ **å¯ä¸­æ–­æ€§**: LLM ä»»åŠ¡å¯ä»¥è¢«å–æ¶ˆ,ASR ä»»åŠ¡ä¸å¯å–æ¶ˆ
- ğŸ”Œ **å›è°ƒè§£è€¦**: LLMExec ä¸ç›´æ¥ä¾èµ– Agent,é€šè¿‡å›è°ƒé€šä¿¡

#### 3.2 äº‹ä»¶æ³¨å†Œæœºåˆ¶

````python name=agent/agent.py (äº‹ä»¶æ³¨å†Œ)
```python
def on(
    self,
    event_type: AgentEvent,
    handler: Callable[[AgentEvent], Awaitable] = None,
):
    """
    æ³¨å†Œäº‹ä»¶å¤„ç†å™¨ (æ”¯æŒä¸¤ç§ç”¨æ³•)
    
    ç”¨æ³• 1: ç›´æ¥æ³¨å†Œ
        agent.on(ASRResultEvent, handler_func)
    
    ç”¨æ³• 2: è£…é¥°å™¨ (é…åˆ @agent_event_handler)
        @agent.on(ASRResultEvent)
        async def handler(event):
            pass
    """
    def decorator(func: Callable[[AgentEvent], Awaitable]):
        self._callbacks.setdefault(event_type, []).append(func)
        return func
    
    if handler is None:
        return decorator  # è¿”å›è£…é¥°å™¨
    else:
        return decorator(handler)  # ç›´æ¥æ³¨å†Œ
```
````

#### 3.3 äº‹ä»¶åˆ†å‘å™¨

````python name=agent/agent.py (äº‹ä»¶åˆ†å‘)
```python
async def _dispatch(self, event: AgentEvent):
    """
    æ ¸å¿ƒåˆ†å‘é€»è¾‘: å°†äº‹ä»¶åˆ†å‘ç»™æ‰€æœ‰æ³¨å†Œçš„å¤„ç†å™¨
    
    å·¥ä½œæµç¨‹:
    1. éå†æ‰€æœ‰æ³¨å†Œçš„äº‹ä»¶ç±»å‹
    2. ä½¿ç”¨ isinstance() æ£€æŸ¥äº‹ä»¶æ˜¯å¦åŒ¹é…
    3. é¡ºåºæ‰§è¡Œæ‰€æœ‰åŒ¹é…çš„å¤„ç†å™¨
    4. é”™è¯¯éš”ç¦»: å•ä¸ªå¤„ç†å™¨å¤±è´¥ä¸å½±å“å…¶ä»–å¤„ç†å™¨
    """
    for etype, handlers in self._callbacks.items():
        if isinstance(event, etype):  # ç±»å‹åŒ¹é…
            for h in handlers:
                try:
                    await h(event)  # å¼‚æ­¥æ‰§è¡Œ
                except asyncio.CancelledError:
                    raise  # ä¸­æ–­ä¿¡å·éœ€è¦å‘ä¸Šä¼ æ’­
                except Exception as e:
                    self.ten_env.log_error(f"Handler error for {etype}: {e}")
```
````

#### 3.4 åŒæ¶ˆè´¹è€…æ¨¡å¼

````python name=agent/agent.py (æ¶ˆè´¹è€…)
```python
# ========== ASR æ¶ˆè´¹è€…: é¡ºåºå¤„ç† ==========
async def _consume_asr(self):
    """
    ASR é˜Ÿåˆ—æ¶ˆè´¹è€… - ç®€å•çš„é¡ºåºå¤„ç†
    ç‰¹ç‚¹: ä¸å¯ä¸­æ–­,ä¿è¯ç”¨æˆ·è¾“å…¥çš„å®Œæ•´æ€§
    """
    while not self.stopped:
        event = await self._asr_queue.get()
        await self._dispatch(event)

# ========== LLM æ¶ˆè´¹è€…: å¯ä¸­æ–­å¤„ç† ==========
async def _consume_llm(self):
    """
    LLM é˜Ÿåˆ—æ¶ˆè´¹è€… - æ”¯æŒä¸­æ–­
    ç‰¹ç‚¹: 
    1. å°†å¤„ç†åŒ…è£…æˆ Task,ä¿å­˜å¼•ç”¨
    2. å¯ä»¥é€šè¿‡ task.cancel() ä¸­æ–­
    3. æ•è· CancelledError,ä¼˜é›…å¤„ç†ä¸­æ–­
    """
    while not self.stopped:
        event = await self._llm_queue.get()
        # ğŸ”‘ å…³é”®: åŒ…è£…æˆ Task
        self._llm_active_task = asyncio.create_task(self._dispatch(event))
        try:
            await self._llm_active_task
        except asyncio.CancelledError:
            self.ten_env.log_info("[Agent] Active LLM task cancelled")
        finally:
            self._llm_active_task = None
```
````

**ä¸ºä»€ä¹ˆ LLM éœ€è¦å¯ä¸­æ–­?**
```
åœºæ™¯: ç”¨æˆ·æ‰“æ–­ AI å›å¤
1. AI æ­£åœ¨è¯´: "ä»Šå¤©å¤©æ°”..."
2. ç”¨æˆ·è¯´: "åœ!æ¢ä¸ªè¯é¢˜"
3. ç³»ç»Ÿéœ€è¦:
   â‘  åœæ­¢ AI ç»§ç»­è¾“å‡º âœ… (å–æ¶ˆ LLM Task)
   â‘¡ æ¸…ç©º TTS é˜Ÿåˆ— âœ… (flush_llm)
   â‘¢ å¼€å§‹å¤„ç†æ–°è¾“å…¥ âœ…
```

#### 3.5 ä¸­æ–­æœºåˆ¶

````python name=agent/agent.py (ä¸­æ–­æœºåˆ¶)
```python
async def flush_llm(self):
    """
    åˆ·æ–° LLM å¤„ç†æµç¨‹ (ç”¨äºä¸­æ–­)
    
    æ­¥éª¤:
    1. è°ƒç”¨ llm_exec.flush() - ä¸­æ­¢ LLM è¯·æ±‚
    2. æ¸…ç©º LLM äº‹ä»¶é˜Ÿåˆ— - ä¸¢å¼ƒæœªå¤„ç†çš„å“åº”
    3. å–æ¶ˆå½“å‰æ´»åŠ¨ä»»åŠ¡ - åœæ­¢æ­£åœ¨æ‰§è¡Œçš„å¤„ç†å™¨
    """
    # Step 1: ä¸­æ­¢ LLM è¯·æ±‚
    await self.llm_exec.flush()
    
    # Step 2: æ¸…ç©ºé˜Ÿåˆ—
    while not self._llm_queue.empty():
        try:
            self._llm_queue.get_nowait()
            self._llm_queue.task_done()
        except asyncio.QueueEmpty:
            break
    
    # Step 3: å–æ¶ˆæ´»åŠ¨ä»»åŠ¡
    if self._llm_active_task and not self._llm_active_task.done():
        self._llm_active_task.cancel()
        try:
            await self._llm_active_task  # ç­‰å¾…ä»»åŠ¡æ¸…ç†
        except asyncio.CancelledError:
            pass
        self._llm_active_task = None
```
````

#### 3.6 å¤–éƒ¨æ¥å£

````python name=agent/agent.py (å¤–éƒ¨æ¥å£)
```python
# ========== æ¥æ”¶å¤–éƒ¨å‘½ä»¤ ==========
async def on_cmd(self, cmd: Cmd):
    """
    å¤„ç†æ¥è‡ª TEN Runtime çš„å‘½ä»¤
    è½¬æ¢ä¸ºæ ‡å‡†äº‹ä»¶: Cmd â†’ AgentEvent
    """
    name = cmd.get_name()
    if name == "on_user_joined":
        await self._emit_direct(UserJoinedEvent())
    elif name == "on_user_left":
        await self._emit_direct(UserLeftEvent())
    elif name == "tool_register":
        tool_json, err = cmd.get_property_to_json("tool")
        tool = LLMToolMetadata.model_validate_json(tool_json)
        await self._emit_direct(
            ToolRegisterEvent(tool=tool, source=cmd.get_source().extension_name)
        )

# ========== æ¥æ”¶å¤–éƒ¨æ•°æ® ==========
async def on_data(self, data: Data):
    """
    å¤„ç†æ¥è‡ª TEN Runtime çš„æ•°æ®
    è½¬æ¢ä¸ºæ ‡å‡†äº‹ä»¶: Data â†’ AgentEvent
    """
    if data.get_name() == "asr_result":
        asr_json, _ = data.get_property_to_json(None)
        asr = json.loads(asr_json)
        await self._emit_asr(
            ASRResultEvent(
                text=asr.get("text", ""),
                final=asr.get("final", False),
                metadata=asr.get("metadata", {}),
            )
        )
```
````

---

### 4. **llm_exec.py** - LLM æ‰§è¡Œå™¨

è¿™æ˜¯ä¸ LLM äº¤äº’çš„æ ¸å¿ƒç»„ä»¶,è´Ÿè´£:
1. **ä¸Šä¸‹æ–‡ç®¡ç†**: ç»´æŠ¤å®Œæ•´çš„å¯¹è¯å†å²
2. **å·¥å…·è°ƒç”¨**: æ”¯æŒ Function Calling
3. **æµå¼å¤„ç†**: å¢é‡å¼è¾“å‡º

#### 4.1 æ ¸å¿ƒæ•°æ®ç»“æ„

````python name=agent/llm_exec.py (æ•°æ®ç»“æ„)
```python
class LLMExec:
    def __init__(self, ten_env: AsyncTenEnv):
        # ========== è¾“å…¥é˜Ÿåˆ— ==========
        self.input_queue = AsyncQueue()  # ç”¨æˆ·è¾“å…¥é˜Ÿåˆ—
        
        # ========== å¯¹è¯ä¸Šä¸‹æ–‡ ==========
        self.contexts: list[LLMMessage] = []
        # ç¤ºä¾‹ç»“æ„:
        # [
        #   {"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·?"},
        #   {"role": "assistant", "content": "ä»Šå¤©åŒ—äº¬æ™´å¤©"},
        #   {"role": "user", "content": "é‚£æ˜å¤©å‘¢?"}
        # ]
        
        # ========== å·¥å…·æ³¨å†Œè¡¨ ==========
        self.available_tools: list[LLMToolMetadata] = []
        self.tool_registry: dict[str, str] = {}  # tool_name â†’ extension_name
        
        # ========== å›è°ƒå‡½æ•° ==========
        self.on_response: Optional[Callable] = None  # æ­£å¸¸å“åº”å›è°ƒ
        self.on_reasoning_response: Optional[Callable] = None  # æ¨ç†å›è°ƒ
        
        # å¯åŠ¨è¾“å…¥é˜Ÿåˆ—å¤„ç†
        self.loop.create_task(self._process_input_queue())
```
````

#### 4.2 è¾“å…¥é˜Ÿåˆ—å¤„ç†

````python name=agent/llm_exec.py (è¾“å…¥å¤„ç†)
```python
async def _process_input_queue(self):
    """
    å¤„ç†ç”¨æˆ·è¾“å…¥é˜Ÿåˆ—
    
    å·¥ä½œæµç¨‹:
    1. ä»é˜Ÿåˆ—å–å‡ºç”¨æˆ·æ–‡æœ¬
    2. åŒ…è£…æˆ LLMMessageContent
    3. å‘é€åˆ° LLM
    4. ç­‰å¾…æµå¼å“åº”
    """
    while not self.stopped:
        try:
            text = await self.input_queue.get()  # é˜»å¡ç­‰å¾…
            new_message = LLMMessageContent(role="user", content=text)
            
            # åˆ›å»ºä»»åŠ¡å¹¶ç­‰å¾…
            self.current_task = self.loop.create_task(
                self._send_to_llm(self.ten_env, new_message)
            )
            await self.current_task
            
        except asyncio.CancelledError:
            # ä¸­æ–­æ—¶çš„æ¸…ç†é€»è¾‘
            text = self.current_text
            self.current_text = None
            if self.on_response and text:
                # å‘é€å½“å‰ç´¯ç§¯çš„æ–‡æœ¬ä½œä¸ºæœ€ç»ˆç»“æœ
                await self.on_response(self.ten_env, "", text, True)
```
````

#### 4.3 å‘é€åˆ° LLM

````python name=agent/llm_exec.py (å‘é€é€»è¾‘)
```python
async def _send_to_llm(
    self, ten_env: AsyncTenEnv, new_message: LLMMessage
) -> None:
    """
    å‘é€æ¶ˆæ¯åˆ° LLM å¹¶å¤„ç†æµå¼å“åº”
    
    æ­¥éª¤:
    1. åˆå¹¶ä¸Šä¸‹æ–‡ + æ–°æ¶ˆæ¯
    2. æ„é€  LLMRequest (åŒ…å«å·¥å…·åˆ—è¡¨)
    3. è°ƒç”¨ LLM Extension
    4. æµå¼å¤„ç†å“åº”
    """
    # Step 1: åˆå¹¶ä¸Šä¸‹æ–‡
    messages = self.contexts.copy()
    messages.append(new_message)
    
    # Step 2: æ„é€ è¯·æ±‚
    request_id = str(uuid.uuid4())
    self.current_request_id = request_id
    llm_input = LLMRequest(
        request_id=request_id,
        messages=messages,
        streaming=True,  # ğŸ”‘ å…³é”®: å¯ç”¨æµå¼è¾“å‡º
        parameters={"temperature": 0.7},
        tools=self.available_tools  # ğŸ”§ ä¼ é€’å·¥å…·åˆ—è¡¨
    )
    
    # Step 3: å‘é€å‘½ä»¤
    response = _send_cmd_ex(ten_env, "chat_completion", "llm", llm_input.model_dump())
    
    # Step 4: å¤„ç†æµå¼å“åº”
    await self._queue_context(ten_env, new_message)  # ä¿å­˜åˆ°ä¸Šä¸‹æ–‡
    
    async for cmd_result, _ in response:
        if cmd_result and not cmd_result.is_final():
            response_json, _ = cmd_result.get_property_to_json(None)
            completion = parse_llm_response(response_json)
            await self._handle_llm_response(completion)  # åˆ†å‘å“åº”
```
````

#### 4.4 å“åº”å¤„ç† (æ¨¡å¼åŒ¹é…)

````python name=agent/llm_exec.py (å“åº”å¤„ç†)
```python
async def _handle_llm_response(self, llm_output: LLMResponse | None):
    """
    å¤„ç† LLM å“åº” - ä½¿ç”¨ Python 3.10+ çš„æ¨¡å¼åŒ¹é…
    
    æ”¯æŒçš„å“åº”ç±»å‹:
    1. MessageDelta: æµå¼æ–‡æœ¬å¢é‡
    2. MessageDone: æ–‡æœ¬å®Œæˆ
    3. ReasoningDelta/Done: æ¨ç†è¿‡ç¨‹ (å¦‚ o1 æ¨¡å‹)
    4. ToolCall: å·¥å…·è°ƒç”¨è¯·æ±‚
    """
    match llm_output:
        # ========== æµå¼æ–‡æœ¬ ==========
        case LLMResponseMessageDelta():
            delta = llm_output.delta  # å¢é‡æ–‡æœ¬
            text = llm_output.content  # ç´¯ç§¯æ–‡æœ¬
            self.current_text = text
            
            if delta and self.on_response:
                # è§¦å‘å›è°ƒ â†’ è½¬æ¢ä¸º LLMResponseEvent
                await self.on_response(self.ten_env, delta, text, False)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            if text:
                await self._write_context(self.ten_env, "assistant", text)
        
        # ========== æ–‡æœ¬å®Œæˆ ==========
        case LLMResponseMessageDone():
            text = llm_output.content
            self.current_text = None
            if self.on_response and text:
                await self.on_response(self.ten_env, "", text, True)
        
        # ========== æ¨ç†è¿‡ç¨‹ ==========
        case LLMResponseReasoningDelta():
            if self.on_reasoning_response:
                await self.on_reasoning_response(
                    self.ten_env, llm_output.delta, llm_output.content, False
                )
        
        # ========== å·¥å…·è°ƒç”¨ ==========
        case LLMResponseToolCall():
            await self._handle_tool_call(llm_output)
```
````

#### 4.5 å·¥å…·è°ƒç”¨å¤„ç†

````python name=agent/llm_exec.py (å·¥å…·è°ƒç”¨)
```python
async def _handle_tool_call(self, llm_output: LLMResponseToolCall):
    """
    å¤„ç†å·¥å…·è°ƒç”¨ (Function Calling)
    
    å·¥ä½œæµç¨‹:
    1. ä» tool_registry æŸ¥æ‰¾å·¥å…·æ‰€åœ¨çš„ extension
    2. å‘é€ tool_call å‘½ä»¤åˆ°ç›®æ ‡ extension
    3. ç­‰å¾…å·¥å…·æ‰§è¡Œç»“æœ
    4. å°†ç»“æœä½œä¸ºæ–°æ¶ˆæ¯å‘é€å› LLM
    5. LLM åŸºäºå·¥å…·ç»“æœç»§ç»­ç”Ÿæˆå›å¤
    """
    self.ten_env.log_info(f"Invoking tool call: {llm_output.name}")
    
    # Step 1: æŸ¥æ‰¾å·¥å…·
    src_extension_name = self.tool_registry.get(llm_output.name)
    
    # Step 2: è°ƒç”¨å·¥å…·
    result, _ = await _send_cmd(
        self.ten_env,
        "tool_call",
        src_extension_name,
        {
            "name": llm_output.name,
            "arguments": llm_output.arguments,
        },
    )
    
    # Step 3: å¤„ç†ç»“æœ
    if result.get_status_code() == StatusCode.OK:
        r, _ = result.get_property_to_json(CMD_PROPERTY_RESULT)
        tool_result: LLMToolResult = json.loads(r)
        
        if tool_result["type"] == "llmresult":
            result_content = tool_result["content"]
            
            # Step 4: ä¿å­˜å·¥å…·è°ƒç”¨åˆ°ä¸Šä¸‹æ–‡
            context_function_call = LLMMessageFunctionCall(
                name=llm_output.name,
                arguments=json.dumps(llm_output.arguments),
                call_id=llm_output.tool_call_id,
            )
            await self._queue_context(self.ten_env, context_function_call)
            
            # Step 5: å°†å·¥å…·ç»“æœå‘é€å› LLM
            await self._send_to_llm(
                self.ten_env,
                LLMMessageFunctionCallOutput(
                    output=result_content,
                    call_id=llm_output.tool_call_id,
                ),
            )
```
````

---

## é›†æˆ RAG/MCP çš„æ¶æ„è®¾è®¡å»ºè®®

æ ¹æ®ä»£ç å®¡æŸ¥å’Œæ‚¨çš„éœ€æ±‚,æˆ‘å»ºè®®ä»¥ä¸‹æ–¹æ¡ˆ:

### æ–¹æ¡ˆ 1: RAG ä½œä¸ºå·¥å…· (æ¨èç”¨äºç®€å•åœºæ™¯)

```python
# æ–°å¢äº‹ä»¶ç±»å‹
class RAGQueryEvent(AgentEventBase):
    """RAG æŸ¥è¯¢äº‹ä»¶"""
    type: Literal["data"] = "data"
    name: Literal["rag_query"] = "rag_query"
    query: str
    context_window: int = 5

class RAGResultEvent(AgentEventBase):
    """RAG æŸ¥è¯¢ç»“æœ"""
    type: Literal["data"] = "data"
    name: Literal["rag_result"] = "rag_result"
    results: List[Dict[str, Any]]
    metadata: Dict[str, Any]
```

**é›†æˆæ­¥éª¤:**
1. åˆ›å»º `rag_tool_python` extension
2. æ³¨å†Œåˆ° LLM çš„å·¥å…·åˆ—è¡¨
3. LLM å†³å®šä½•æ—¶è°ƒç”¨ RAG
4. å·¥å…·è¿”å›ç»“æœå,LLM åŸºäºç»“æœç”Ÿæˆå›å¤

### æ–¹æ¡ˆ 2: RAG ä½œä¸ºå‰ç½®å¢å¼º (æ¨èç”¨äºå¤æ‚åœºæ™¯)

åˆ›å»ºæ–°æ–‡ä»¶å»ºè®®å‚è€ƒæˆ‘ä¹‹å‰æåˆ°çš„ä»£ç è´¨é‡è§„åˆ™,æˆ‘å‘ç°å½“å‰ä»£ç å­˜åœ¨ä»¥ä¸‹é—®é¢˜:è®©æˆ‘è¯¦ç»†è§£é‡Š Agent æ¨¡å—çš„ä»£ç æ¶æ„å’Œä½œç”¨,å¹¶ä¸ºä½ è§„åˆ’ RAG/MCP çš„é›†æˆæ–¹æ¡ˆã€‚

---

## Agent æ¨¡å—è¯¦ç»†è§£æ

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MainControlExtension                     â”‚
â”‚  (extension.py - äº‹ä»¶è·¯ç”±å™¨)                                  â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            Agent (agent.py)                           â”‚   â”‚
â”‚  â”‚  - äº‹ä»¶é˜Ÿåˆ—ç®¡ç† (_asr_queue, _llm_queue)              â”‚   â”‚
â”‚  â”‚  - äº‹ä»¶åˆ†å‘å™¨ (_dispatch)                             â”‚   â”‚
â”‚  â”‚  - å›è°ƒæ³¨å†Œè¡¨ (_callbacks)                            â”‚   â”‚
â”‚  â”‚                                                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚    LLMExec (llm_exec.py)                     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - è¾“å…¥é˜Ÿåˆ— (input_queue)                    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - ä¸Šä¸‹æ–‡ç®¡ç† (contexts[])                   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - å·¥å…·æ³¨å†Œè¡¨ (tool_registry)                â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - æµå¼å¤„ç† (_handle_llm_response)           â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. **MainControlExtension (extension.py)**

**èŒè´£**: ä½œä¸º TEN Framework æ‰©å±•çš„å…¥å£ç‚¹,è´Ÿè´£äº‹ä»¶è·¯ç”±

```python
class MainControlExtension(AsyncExtension):
    """
    ğŸ¯ æ ¸å¿ƒèŒè´£:
    1. åˆå§‹åŒ– Agent å’Œé…ç½®
    2. å°† TEN è¿è¡Œæ—¶äº‹ä»¶ (on_cmd/on_data) è·¯ç”±åˆ° Agent
    3. ç®¡ç†ä¼šè¯çŠ¶æ€ (ç”¨æˆ·æ•°é‡ã€è½®æ¬¡ID)
    4. å¤„ç† TTS å’Œè½¬å½•è¾“å‡º
    """
    
    # ğŸ”§ å…³é”®å±æ€§
    agent: Agent                    # å†…éƒ¨ä»£ç†
    config: MainControlConfig       # é…ç½®
    _rtc_user_count: int            # å½“å‰ç”¨æˆ·æ•°
    turn_id: int                    # å¯¹è¯è½®æ¬¡
    sentence_fragment: str          # å¥å­ç‰‡æ®µç¼“å­˜
```

**å…³é”®æ–¹æ³•:**

#### `on_init` - åˆå§‹åŒ–
```python
async def on_init(self, ten_env: AsyncTenEnv):
    """
    1. åŠ è½½é…ç½®
    2. åˆ›å»º Agent å®ä¾‹
    3. è‡ªåŠ¨æ³¨å†Œè£…é¥°å™¨æ ‡æ³¨çš„äº‹ä»¶å¤„ç†å™¨
    """
    self.agent = Agent(ten_env)
    
    # é€šè¿‡åå°„æ‰¾åˆ°æ‰€æœ‰ @agent_event_handler è£…é¥°çš„æ–¹æ³•
    for attr_name in dir(self):
        fn = getattr(self, attr_name)
        event_type = getattr(fn, "_agent_event_type", None)
        if event_type:
            self.agent.on(event_type, fn)  # æ³¨å†Œåˆ° Agent
```

#### äº‹ä»¶å¤„ç†å™¨ (ä½¿ç”¨è£…é¥°å™¨)

```python
@agent_event_handler(UserJoinedEvent)
async def _on_user_joined(self, event: UserJoinedEvent):
    """ç”¨æˆ·åŠ å…¥æ—¶å‘é€æ¬¢è¿è¯­"""
    self._rtc_user_count += 1
    if self._rtc_user_count == 1:
        await self._send_to_tts(self.config.greeting, True)

@agent_event_handler(ASRResultEvent)
async def _on_asr_result(self, event: ASRResultEvent):
    """
    å¤„ç†è¯­éŸ³è¯†åˆ«ç»“æœ
    
    æµç¨‹:
    1. å¦‚æœæ˜¯æœ€ç»ˆç»“æœ â†’ å‘é€åˆ° LLM
    2. å¦‚æœæ–‡æœ¬é•¿åº¦ > 2 æˆ–æ˜¯æœ€ç»ˆç»“æœ â†’ è§¦å‘ä¸­æ–­
    3. å‘é€è½¬å½•åˆ° message_collector
    """
    if event.final or len(event.text) > 2:
        await self._interrupt()  # ğŸ”¥ æ‰“æ–­å½“å‰ AI å›å¤
    
    if event.final:
        self.turn_id += 1
        await self.agent.queue_llm_input(event.text)  # ğŸš€ å‘é€åˆ° LLM

@agent_event_handler(LLMResponseEvent)
async def _on_llm_response(self, event: LLMResponseEvent):
    """
    å¤„ç† LLM å“åº”
    
    æµç¨‹:
    1. æµå¼è¾“å‡ºæ—¶ â†’ æŒ‰å¥å­åˆ†å‰²å¹¶å‘é€åˆ° TTS
    2. æœ€ç»ˆè¾“å‡ºæ—¶ â†’ å‘é€å‰©ä½™æ–‡æœ¬åˆ° TTS
    """
    if not event.is_final:
        # ä½¿ç”¨ parse_sentences æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²å¥å­
        sentences, self.sentence_fragment = parse_sentences(
            self.sentence_fragment, event.delta
        )
        for s in sentences:
            await self._send_to_tts(s, False)  # ğŸ’¬ é€å¥æ’­æ”¾
```

#### æ ¸å¿ƒè¾…åŠ©æ–¹æ³•

```python
async def _interrupt(self):
    """
    ğŸ›‘ ä¸­æ–­å½“å‰å¯¹è¯
    
    è§¦å‘æ—¶æœº: ç”¨æˆ·å¼€å§‹è¯´è¯æ—¶
    
    æ‰§è¡Œæ“ä½œ:
    1. æ¸…ç©ºå¥å­ç‰‡æ®µç¼“å­˜
    2. æ¸…ç©º LLM é˜Ÿåˆ—
    3. æ¸…ç©º TTS é˜Ÿåˆ—
    4. æ¸…ç©º RTC éŸ³é¢‘ç¼“å†²åŒº
    """
    self.sentence_fragment = ""
    await self.agent.flush_llm()
    await _send_data(self.ten_env, "tts_flush", "tts", {...})
    await _send_cmd(self.ten_env, "flush", "agora_rtc")

async def _send_to_tts(self, text: str, is_final: bool):
    """å‘é€æ–‡æœ¬åˆ° TTS ç³»ç»Ÿ"""
    await _send_data(
        self.ten_env,
        "tts_text_input",
        "tts",
        {
            "request_id": f"tts-request-{self.turn_id}",
            "text": text,
            "text_input_end": is_final,
        },
    )
```

---

### 2. **Agent (agent/agent.py)**

**èŒè´£**: äº‹ä»¶é©±åŠ¨çš„è°ƒåº¦æ ¸å¿ƒ,ç®¡ç† ASR å’Œ LLM äº‹ä»¶é˜Ÿåˆ—

```python
class Agent:
    """
    ğŸ¯ æ ¸å¿ƒèŒè´£:
    1. ç»´æŠ¤ä¸¤ä¸ªç‹¬ç«‹çš„äº‹ä»¶é˜Ÿåˆ— (ASR/LLM)
    2. æä¾›äº‹ä»¶æ³¨å†Œæœºåˆ¶ (è§‚å¯Ÿè€…æ¨¡å¼)
    3. è°ƒåº¦ LLMExec å¤„ç† LLM è¯·æ±‚
    4. æ”¯æŒå¯ä¸­æ–­çš„ LLM ä»»åŠ¡
    """
    
    # ğŸ”§ å…³é”®å±æ€§
    _callbacks: dict[AgentEvent, list[Callable]]  # äº‹ä»¶å›è°ƒæ³¨å†Œè¡¨
    _asr_queue: asyncio.Queue[ASRResultEvent]     # ASR äº‹ä»¶é˜Ÿåˆ—
    _llm_queue: asyncio.Queue[LLMResponseEvent]   # LLM äº‹ä»¶é˜Ÿåˆ—
    llm_exec: LLMExec                             # LLM æ‰§è¡Œå™¨
    _llm_active_task: Optional[asyncio.Task]      # å½“å‰æ´»è·ƒçš„ LLM ä»»åŠ¡
```

#### äº‹ä»¶æ³¨å†Œæœºåˆ¶

```python
def on(self, event_type: AgentEvent, handler: Callable):
    """
    æ³¨å†Œäº‹ä»¶å¤„ç†å™¨ (è§‚å¯Ÿè€…æ¨¡å¼)
    
    æ”¯æŒä¸¤ç§ç”¨æ³•:
    1. agent.on(ASRResultEvent, async_handler)
    2. @agent.on(ASRResultEvent)
       async def handler(event): ...
    """
    def decorator(func: Callable):
        self._callbacks.setdefault(event_type, []).append(func)
        return func
    
    if handler is None:
        return decorator
    else:
        return decorator(handler)

async def _dispatch(self, event: AgentEvent):
    """åˆ†å‘äº‹ä»¶åˆ°æ‰€æœ‰æ³¨å†Œçš„å¤„ç†å™¨"""
    for etype, handlers in self._callbacks.items():
        if isinstance(event, etype):
            for h in handlers:
                await h(event)  # ğŸš€ è°ƒç”¨å¤„ç†å™¨
```

#### åŒé˜Ÿåˆ—æ¶ˆè´¹è€…

```python
async def _consume_asr(self):
    """
    ASR äº‹ä»¶æ¶ˆè´¹è€…
    
    ç‰¹ç‚¹: é¡ºåºå¤„ç†,ä¸å¯ä¸­æ–­
    """
    while not self.stopped:
        event = await self._asr_queue.get()
        await self._dispatch(event)

async def _consume_llm(self):
    """
    LLM äº‹ä»¶æ¶ˆè´¹è€…
    
    ç‰¹ç‚¹: å¯ä¸­æ–­ (ç”¨äºå®ç°æ‰“æ–­åŠŸèƒ½)
    """
    while not self.stopped:
        event = await self._llm_queue.get()
        
        # ä½œä¸ºç‹¬ç«‹ä»»åŠ¡è¿è¡Œ,å¯ä»¥è¢«å–æ¶ˆ
        self._llm_active_task = asyncio.create_task(self._dispatch(event))
        
        try:
            await self._llm_active_task
        except asyncio.CancelledError:
            self.ten_env.log_info("[Agent] LLM task cancelled")
        finally:
            self._llm_active_task = None
```

#### TEN è¿è¡Œæ—¶æ¥å£

```python
async def on_cmd(self, cmd: Cmd):
    """
    å¤„ç†æ¥è‡ª TEN è¿è¡Œæ—¶çš„å‘½ä»¤
    
    æ”¯æŒçš„å‘½ä»¤:
    - on_user_joined: ç”¨æˆ·åŠ å…¥
    - on_user_left: ç”¨æˆ·ç¦»å¼€
    - tool_register: å·¥å…·æ³¨å†Œ
    """
    name = cmd.get_name()
    if name == "on_user_joined":
        await self._emit_direct(UserJoinedEvent())
    elif name == "tool_register":
        tool_json, _ = cmd.get_property_to_json("tool")
        tool = LLMToolMetadata.model_validate_json(tool_json)
        await self._emit_direct(ToolRegisterEvent(tool=tool, source=...))

async def on_data(self, data: Data):
    """
    å¤„ç†æ¥è‡ª TEN è¿è¡Œæ—¶çš„æ•°æ®
    
    æ”¯æŒçš„æ•°æ®:
    - asr_result: è¯­éŸ³è¯†åˆ«ç»“æœ
    """
    if data.get_name() == "asr_result":
        asr_json, _ = data.get_property_to_json(None)
        asr = json.loads(asr_json)
        await self._emit_asr(ASRResultEvent(
            text=asr["text"],
            final=asr["final"],
            metadata=asr.get("metadata", {})
        ))
```

#### LLM æ§åˆ¶æ¥å£

```python
async def queue_llm_input(self, text: str):
    """é˜Ÿåˆ—åŒ–ç”¨æˆ·è¾“å…¥åˆ° LLM"""
    await self.llm_exec.queue_input(text)

async def flush_llm(self):
    """
    ğŸ›‘ æ¸…ç©º LLM é˜Ÿåˆ—å’Œä»»åŠ¡
    
    æ‰§è¡Œæ“ä½œ:
    1. è°ƒç”¨ llm_exec.flush() ä¸­æ­¢å½“å‰è¯·æ±‚
    2. æ¸…ç©º _llm_queue
    3. å–æ¶ˆ _llm_active_task
    """
    await self.llm_exec.flush()
    
    # æ¸…ç©ºé˜Ÿåˆ—
    while not self._llm_queue.empty():
        self._llm_queue.get_nowait()
    
    # å–æ¶ˆæ´»è·ƒä»»åŠ¡
    if self._llm_active_task and not self._llm_active_task.done():
        self._llm_active_task.cancel()
        await self._llm_active_task  # ç­‰å¾…å–æ¶ˆå®Œæˆ
```

---

### 3. **LLMExec (agent/llm_exec.py)**

**èŒè´£**: LLM æ‰§è¡Œå™¨,è´Ÿè´£ä¸ LLM çš„äº¤äº’

```python
class LLMExec:
    """
    ğŸ¯ æ ¸å¿ƒèŒè´£:
    1. ç®¡ç† LLM è¾“å…¥é˜Ÿåˆ—
    2. ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡
    3. å¤„ç†æµå¼å“åº”
    4. æ”¯æŒå·¥å…·è°ƒç”¨ (Function Calling)
    """
    
    # ğŸ”§ å…³é”®å±æ€§
    input_queue: AsyncQueue                       # è¾“å…¥é˜Ÿåˆ—
    contexts: list[LLMMessage]                    # å¯¹è¯ä¸Šä¸‹æ–‡
    tool_registry: dict[str, str]                 # å·¥å…·æ³¨å†Œè¡¨
    available_tools: list[LLMToolMetadata]        # å¯ç”¨å·¥å…·åˆ—è¡¨
    current_request_id: Optional[str]             # å½“å‰è¯·æ±‚ID
    on_response: Callable                         # å“åº”å›è°ƒ
    on_reasoning_response: Callable               # æ¨ç†å“åº”å›è°ƒ
```

#### è¾“å…¥é˜Ÿåˆ—å¤„ç†

```python
async def _process_input_queue(self):
    """
    å¤„ç†è¾“å…¥é˜Ÿåˆ—
    
    æµç¨‹:
    1. ä»é˜Ÿåˆ—å–å‡ºç”¨æˆ·è¾“å…¥
    2. æ„é€  LLMMessageContent
    3. è°ƒç”¨ _send_to_llm å‘é€è¯·æ±‚
    4. å¤„ç†å–æ¶ˆå¼‚å¸¸ (ä¸­æ–­æ—¶)
    """
    while not self.stopped:
        try:
            text = await self.input_queue.get()
            new_message = LLMMessageContent(role="user", content=text)
            
            self.current_task = self.loop.create_task(
                self._send_to_llm(self.ten_env, new_message)
            )
            await self.current_task
            
        except asyncio.CancelledError:
            # ğŸ”¥ å¤„ç†ä¸­æ–­: å°†å½“å‰æ–‡æœ¬æ ‡è®°ä¸ºå®Œæˆ
            text = self.current_text
            if self.on_response and text:
                await self.on_response(self.ten_env, "", text, True)
```

#### å‘é€åˆ° LLM

```python
async def _send_to_llm(self, ten_env: AsyncTenEnv, new_message: LLMMessage):
    """
    å‘é€è¯·æ±‚åˆ° LLM
    
    æµç¨‹:
    1. å¤åˆ¶ä¸Šä¸‹æ–‡å¹¶æ·»åŠ æ–°æ¶ˆæ¯
    2. æ„é€  LLMRequest (åŒ…å«å·¥å…·åˆ—è¡¨)
    3. è°ƒç”¨ send_cmd_ex å‘é€å‘½ä»¤ (æµå¼)
    4. é€ä¸ªå¤„ç†å“åº”ç‰‡æ®µ
    """
    messages = self.contexts.copy()
    messages.append(new_message)
    
    request_id = str(uuid.uuid4())
    self.current_request_id = request_id
    
    llm_input = LLMRequest(
        request_id=request_id,
        messages=messages,
        streaming=True,  # ğŸŒŠ æµå¼è¾“å‡º
        tools=self.available_tools,  # ğŸ”§ å·¥å…·åˆ—è¡¨
    )
    
    response = _send_cmd_ex(ten_env, "chat_completion", "llm", llm_input.model_dump())
    
    # é˜Ÿåˆ—åŒ–æ–°æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
    await self._queue_context(ten_env, new_message)
    
    # ğŸ”„ æµå¼å¤„ç†å“åº”
    async for cmd_result, _ in response:
        if cmd_result and not cmd_result.is_final():
            response_json, _ = cmd_result.get_property_to_json(None)
            completion = parse_llm_response(response_json)
            await self._handle_llm_response(completion)
```

#### å“åº”å¤„ç† (æ”¯æŒå¤šç§ç±»å‹)

```python
async def _handle_llm_response(self, llm_output: LLMResponse):
    """
    å¤„ç† LLM å“åº”
    
    æ”¯æŒçš„å“åº”ç±»å‹:
    1. LLMResponseMessageDelta - æµå¼æ–‡æœ¬ç‰‡æ®µ
    2. LLMResponseMessageDone - æ–‡æœ¬ç”Ÿæˆå®Œæˆ
    3. LLMResponseReasoningDelta - æ¨ç†è¿‡ç¨‹ç‰‡æ®µ (å¦‚ o1 æ¨¡å‹)
    4. LLMResponseReasoningDone - æ¨ç†å®Œæˆ
    5. LLMResponseToolCall - å·¥å…·è°ƒç”¨
    """
    match llm_output:
        case LLMResponseMessageDelta():
            # ğŸŒŠ æµå¼æ–‡æœ¬
            delta = llm_output.delta
            text = llm_output.content
            self.current_text = text
            
            if delta and self.on_response:
                await self.on_response(self.ten_env, delta, text, False)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            await self._write_context(self.ten_env, "assistant", text)
        
        case LLMResponseMessageDone():
            # âœ… æ–‡æœ¬å®Œæˆ
            text = llm_output.content
            if self.on_response and text:
                await self.on_response(self.ten_env, "", text, True)
        
        case LLMResponseToolCall():
            # ğŸ”§ å·¥å…·è°ƒç”¨
            src_extension = self.tool_registry.get(llm_output.name)
            
            # è°ƒç”¨å·¥å…·æ‰©å±•
            result, _ = await _send_cmd(
                self.ten_env,
                "tool_call",
                src_extension,
                {"name": llm_output.name, "arguments": llm_output.arguments},
            )
            
            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            if result.get_status_code() == StatusCode.OK:
                tool_result = json.loads(result.get_property_to_json("result"))
                
                # æ·»åŠ  function_call æ¶ˆæ¯
                await self._queue_context(
                    self.ten_env,
                    LLMMessageFunctionCall(
                        name=llm_output.name,
                        arguments=json.dumps(llm_output.arguments),
                        call_id=llm_output.tool_call_id,
                    )
                )
                
                # æ·»åŠ  function_call_output å¹¶ç»§ç»­å¯¹è¯
                await self._send_to_llm(
                    self.ten_env,
                    LLMMessageFunctionCallOutput(
                        output=tool_result["content"],
                        call_id=llm_output.tool_call_id,
                    ),
                )
```

#### ä¸Šä¸‹æ–‡ç®¡ç†

```python
async def _write_context(
    self, ten_env: AsyncTenEnv, role: Literal["user", "assistant"], content: str
):
    """
    å†™å…¥ä¸Šä¸‹æ–‡
    
    é€»è¾‘:
    - å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯çš„ role ç›¸åŒ â†’ æ›´æ–°å†…å®¹
    - å¦åˆ™ â†’ æ·»åŠ æ–°æ¶ˆæ¯
    """
    last_context = self.contexts[-1] if self.contexts else None
    if last_context and last_context.role == role:
        last_context.content = content  # æ›´æ–°
    else:
        new_message = LLMMessageContent(role=role, content=content)
        await self._queue_context(ten_env, new_message)  # æ·»åŠ 
```

---

### 4. **äº‹ä»¶å®šä¹‰ (agent/events.py)**

```python
# åŸºç±»
class AgentEventBase(BaseModel):
    type: Literal["cmd", "data"]
    name: str

# å‘½ä»¤äº‹ä»¶
class UserJoinedEvent(AgentEventBase):
    type: Literal["cmd"] = "cmd"
    name: Literal["on_user_joined"] = "on_user_joined"

class UserLeftEvent(AgentEventBase):
    type: Literal["cmd"] = "cmd"
    name: Literal["on_user_left"] = "on_user_left"

class ToolRegisterEvent(AgentEventBase):
    type: Literal["cmd"] = "cmd"
    name: Literal["tool_register"] = "tool_register"
    tool: LLMToolMetadata
    source: str

# æ•°æ®äº‹ä»¶
class ASRResultEvent(AgentEventBase):
    type: Literal["data"] = "data"
    name: Literal["asr_result"] = "asr_result"
    text: str
    final: bool
    metadata: Dict[str, Any]

class LLMResponseEvent(AgentEventBase):
    type: Literal["message", "reasoning"] = "message"
    name: Literal["llm_response"] = "llm_response"
    delta: str       # å¢é‡æ–‡æœ¬
    text: str        # å®Œæ•´æ–‡æœ¬
    is_final: bool   # æ˜¯å¦ç»“æŸ

# è”åˆç±»å‹
AgentEvent = Union[
    UserJoinedEvent,
    UserLeftEvent,
    ToolRegisterEvent,
    ASRResultEvent,
    LLMResponseEvent,
]
```

---

## é›†æˆ RAG/MCP çš„æ–¹æ¡ˆ

æ ¹æ®ä¸Šé¢çš„åˆ†æ,æˆ‘ä¸ºä½ è®¾è®¡äº†ä¸€ä¸ªä¼˜é›…çš„é›†æˆæ–¹æ¡ˆ:

### æ–¹æ¡ˆ 1: **RAG ä½œä¸º LLM å‰ç½®å¢å¼ºå™¨**

**æ¶æ„:**
```
ç”¨æˆ·è¾“å…¥ â†’ RAGExecutor.enrich() â†’ LLMExec._send_to_llm()
```

**å®ç°æ­¥éª¤:**

#### 1. æ–°å¢ `agent/rag/` ç›®å½•

```python
# agent/rag/rag_executor.py
class RAGExecutor:
    """RAG æŸ¥è¯¢å¢å¼ºå™¨"""
    
    def __init__(self, ten_env: AsyncTenEnv, retriever: BaseRetriever):
        self.ten_env = ten_env
        self.retriever = retriever
        self.context_enricher = ContextEnricher()
    
    async def enrich(self, query: str, history: List[LLMMessage]) -> str:
        """
        å¢å¼ºç”¨æˆ·æŸ¥è¯¢
        
        æµç¨‹:
        1. æå–æŸ¥è¯¢æ„å›¾
        2. æ£€ç´¢ç›¸å…³æ–‡æ¡£ (Top-K)
        3. æ ¼å¼åŒ–å¢å¼ºä¸Šä¸‹æ–‡
        """
        # æå–æ„å›¾
        intent = self._extract_intent(query, history)
        
        # æ£€ç´¢
        results = await self.retriever.retrieve(intent, top_k=3)
        
        # æ ¼å¼åŒ–
        enriched = self.context_enricher.format(query, results)
        
        self.ten_env.log_info(f"[RAG] Original: {query}")
        self.ten_env.log_info(f"[RAG] Enriched: {enriched}")
        
        return enriched
    
    def _extract_intent(self, query: str, history: List[LLMMessage]) -> str:
        """æå–æŸ¥è¯¢æ„å›¾ (è€ƒè™‘å†å²)"""
        if not history:
            return query
        
        # ç®€å•ç­–ç•¥: ç»“åˆæœ€åä¸€è½®å¯¹è¯
        last_assistant_msg = next(
            (msg for msg in reversed(history) if msg.role == "assistant"),
            None
        )
        
        if last_assistant_msg:
            return f"ä¸Šä¸‹æ–‡: {last_assistant_msg.content}\né—®é¢˜: {query}"
        else:
            return query
```

```python
# agent/rag/retriever.py
from abc import ABC, abstractmethod

class BaseRetriever(ABC):
    @abstractmethod
    async def retrieve(self, query: str, top_k: int = 5) -> List[RAGResult]:
        pass

@dataclass
class RAGResult:
    content: str
    score: float
    metadata: Dict[str, Any]

class VectorRetriever(BaseRetriever):
    """å‘é‡æ£€ç´¢å™¨ (FAISS/Milvus/ChromaDB)"""
    
    def __init__(self, index_path: str, embedding_model: str = "text-embedding-ada-002"):
        self.index = self._load_index(index_path)
        self.embedding_model = embedding_model
    
    async def retrieve(self, query: str, top_k: int = 5) -> List[RAGResult]:
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = await self._embed(query)
        
        # 2. å‘é‡æ£€ç´¢
        results = self.index.search(query_embedding, top_k)
        
        # 3. æ ¼å¼åŒ–ç»“æœ
        return [
            RAGResult(
                content=res["text"],
                score=res["score"],
                metadata=res["metadata"]
            )
            for res in results
        ]
```

```python
# agent/rag/context_enricher.py
class ContextEnricher:
    """ä¸Šä¸‹æ–‡å¢å¼ºå™¨"""
    
    def format(self, query: str, results: List[RAGResult]) -> str:
        """
        æ ¼å¼åŒ–å¢å¼ºä¸Šä¸‹æ–‡
        
        ç­–ç•¥: å°†æ£€ç´¢ç»“æœä½œä¸º"å‚è€ƒèµ„æ–™"é™„åŠ åˆ°æŸ¥è¯¢å‰
        """
        if not results:
            return query
        
        context_parts = ["# å‚è€ƒèµ„æ–™\n"]
        for i, res in enumerate(results, 1):
            context_parts.append(f"## èµ„æ–™ {i} (ç›¸å…³åº¦: {res.score:.2f})\n")
            context_parts.append(f"{res.content}\n\n")
        
        context_parts.append(f"# ç”¨æˆ·é—®é¢˜\n{query}")
        
        return "".join(context_parts)
```

#### 2. ä¿®æ”¹ `LLMExec._process_input_queue()`

```python
class LLMExec:
    def __init__(self, ten_env: AsyncTenEnv):
        # åŸæœ‰å±æ€§...
        
        # ğŸ†• RAG å¢å¼ºå™¨ (å¯é€‰)
        self.rag_executor: Optional[RAGExecutor] = None
    
    async def _process_input_queue(self):
        while not self.stopped:
            try:
                text = await self.input_queue.get()
                
                # ğŸ†• RAG å‰ç½®å¢å¼º
                if self.rag_executor:
                    enriched_text = await self.rag_executor.enrich(
                        text, self.contexts
                    )
                else:
                    enriched_text = text
                
                new_message = LLMMessageContent(role="user", content=enriched_text)
                self.current_task = self.loop.create_task(
                    self._send_to_llm(self.ten_env, new_message)
                )
                await self.current_task
            except asyncio.CancelledError:
                # å¤„ç†å–æ¶ˆ...
                pass
```

#### 3. é…ç½®å’Œåˆå§‹åŒ–

```python
# extension.py
async def on_init(self, ten_env: AsyncTenEnv):
    # åŠ è½½é…ç½®
    config_json, _ = await ten_env.get_property_to_json(None)
    self.config = MainControlConfig.model_validate_json(config_json)
    
    # åˆ›å»º Agent
    self.agent = Agent(ten_env)
    
    # ğŸ†• å¯é€‰: å¯ç”¨ RAG
    if self.config.enable_rag:
        ten_env.log_info("[MainControl] Initializing RAG...")
        
        retriever = VectorRetriever(
            index_path=self.config.rag_index_path,
            embedding_model=self.config.rag_embedding_model
        )
        
        rag_executor = RAGExecutor(ten_env, retriever)
        self.agent.llm_exec.rag_executor = rag_executor
        
        ten_env.log_info("[MainControl] RAG initialized")
    
    # æ³¨å†Œäº‹ä»¶å¤„ç†å™¨...
```

```json
// property.json é…ç½®
{
  "greeting": "ä½ å¥½!æˆ‘æ˜¯è¯­éŸ³åŠ©æ‰‹ã€‚",
  "enable_rag": true,
  "rag_index_path": "/path/to/vector_index",
  "rag_embedding_model": "text-embedding-ada-002"
}
```

---

### æ–¹æ¡ˆ 2: **MCP ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨**

**æ¶æ„:**
```
LLMExec.contexts â†’ MCPContextManager.sync() â†’ MCP Server
                    â†‘ æŒä¹…åŒ–åˆ°å¤–éƒ¨å­˜å‚¨
```

**å®ç°æ­¥éª¤:**

#### 1. æ–°å¢ `agent/mcp/` ç›®å½•

```python
# agent/mcp/context_manager.py
class MCPContextManager:
    """Model Context Protocol ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, ten_env: AsyncTenEnv, server_url: str):
        self.ten_env = ten_env
        self.client = MCPClient(server_url)
        self.session_id: Optional[str] = None
    
    async def create_session(self, metadata: Dict[str, Any]) -> str:
        """åˆ›å»ºæ–°ä¼šè¯"""
        response = await self.client.create_context({
            "type": "conversation",
            "metadata": metadata
        })
        self.session_id = response["context_id"]
        return self.session_id
    
    async def sync_messages(self, messages: List[LLMMessage]):
        """åŒæ­¥æ¶ˆæ¯åˆ° MCP Server"""
        if not self.session_id:
            raise RuntimeError("Session not created")
        
        await self.client.update_context(
            self.session_id,
            {
                "messages": [msg.model_dump() for msg in messages],
                "updated_at": int(time.time())
            }
        )
    
    async def get_history(self, limit: int = 10) -> List[LLMMessage]:
        """ä» MCP Server è·å–å†å²"""
        if not self.session_id:
            return []
        
        response = await self.client.get_context(self.session_id)
        messages_data = response.get("messages", [])
        
        return [LLMMessage.model_validate(msg) for msg in messages_data[-limit:]]
```

```python
# agent/mcp/client.py
class MCPClient:
    """MCP åè®®å®¢æˆ·ç«¯"""
    
    def __init__(self, server_url: str):
        self.server_url = server_url
        self.session = aiohttp.ClientSession()
    
    async def create_context(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºä¸Šä¸‹æ–‡"""
        async with self.session.post(
            f"{self.server_url}/contexts",
            json=data
        ) as resp:
            return await resp.json()
    
    async def update_context(self, context_id: str, updates: Dict[str, Any]):
        """æ›´æ–°ä¸Šä¸‹æ–‡"""
        async with self.session.patch(
            f"{self.server_url}/contexts/{context_id}",
            json=updates
        ) as resp:
            return await resp.json()
    
    async def get_context(self, context_id: str) -> Dict[str, Any]:
        """è·å–ä¸Šä¸‹æ–‡"""
        async with self.session.get(
            f"{self.server_url}/contexts/{context_id}"
        ) as resp:
            return await resp.json()
```

#### 2. ä¿®æ”¹ `LLMExec` é›†æˆ MCP

```python
class LLMExec:
    def __init__(self, ten_env: AsyncTenEnv):
        # åŸæœ‰å±æ€§...
        
        # ğŸ†• MCP ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (å¯é€‰)
        self.mcp_manager: Optional[MCPContextManager] = None
    
    async def _queue_context(self, ten_env: AsyncTenEnv, new_message: LLMMessage):
        """é˜Ÿåˆ—åŒ–æ–°æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡"""
        ten_env.log_info(f"_queue_context: {new_message}")
        self.contexts.append(new_message)
        
        # ğŸ†• åŒæ­¥åˆ° MCP Server
        if self.mcp_manager:
            await self.mcp_manager.sync_messages(self.contexts)
```

#### 3. åˆå§‹åŒ– MCP

```python
# extension.py
async def on_init(self, ten_env: AsyncTenEnv):
    # ... åŠ è½½é…ç½® ...
    
    self.agent = Agent(ten_env)
    
    # ğŸ†• å¯é€‰: å¯ç”¨ MCP
    if self.config.enable_mcp:
        ten_env.log_info("[MainControl] Initializing MCP...")
        
        mcp_manager = MCPContextManager(
            ten_env,
            server_url=self.config.mcp_server_url
        )
        
        # åˆ›å»ºä¼šè¯
        session_id = await mcp_manager.create_session({
            "user_id": "default_user",
            "created_at": int(time.time())
        })
        
        self.agent.llm_exec.mcp_manager = mcp_manager
        
        ten_env.log_info(f"[MainControl] MCP session created: {session_id}")
```

## å¯¹æ¯”ä¸¤ç§æ–¹æ¡ˆ

| ç‰¹æ€§         | RAG æ–¹æ¡ˆ          | MCP æ–¹æ¡ˆ            |
| ------------ | ----------------- | ------------------- |
| **é›†æˆä½ç½®** | LLM å‰ç½®          | ä¸Šä¸‹æ–‡åŒæ­¥          |
| **ä¸»è¦ä½œç”¨** | å¢å¼ºæŸ¥è¯¢,è¡¥å……çŸ¥è¯† | æŒä¹…åŒ–ä¸Šä¸‹æ–‡,è·¨ä¼šè¯ |
| **æ€§èƒ½å½±å“** | æ¯æ¬¡æŸ¥è¯¢å¢åŠ å»¶è¿Ÿ  | å¼‚æ­¥åŒæ­¥,å½±å“å°     |
| **å¤æ‚åº¦**   | ä¸­ç­‰ (éœ€è¦å‘é‡åº“) | ä½ (HTTP è¯·æ±‚)      |
| **é€‚ç”¨åœºæ™¯** | çŸ¥è¯†å¯†é›†å‹å¯¹è¯    | å¤šè½®é•¿å¯¹è¯,ç”¨æˆ·å†å² |

## æ¨èæ–¹æ¡ˆ

**å»ºè®®åŒæ—¶é›†æˆ RAG å’Œ MCP**,å› ä¸ºå®ƒä»¬è§£å†³ä¸åŒé—®é¢˜:

1. **RAG**: åœ¨ç”¨æˆ·æé—®æ—¶,å®æ—¶æ£€ç´¢ç›¸å…³çŸ¥è¯†
2. **MCP**: æŒä¹…åŒ–å¯¹è¯å†å²,æ”¯æŒè·¨ä¼šè¯æ¢å¤

**é›†æˆé¡ºåº:**
1. å…ˆå®ç° RAG (æ›´æ ¸å¿ƒ,æå‡å›ç­”è´¨é‡)
2. å†å®ç° MCP (å¢å¼ºåŠŸèƒ½,æ”¯æŒé•¿æœŸè®°å¿†)

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨:**
è¯·å‘Šè¯‰æˆ‘ä½ å¸Œæœ›:
1. æˆ‘åˆ›å»ºå®Œæ•´çš„ä»£ç é‡æ„ PR (åŒ…å« RAG/MCP)?
2. è¿˜æ˜¯å…ˆæä¾›å•ä¸ªæ¨¡å—çš„è¯¦ç»†å®ç°ç¤ºä¾‹?
3. æˆ–è€…ä½ æœ‰å…¶ä»–ç‰¹å®šéœ€æ±‚?